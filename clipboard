Thought for 19s

Got it—here’s a practical, end-to-end way to (a) get a heap dump on OOME, and (b) store it on persistent storage in OpenShift.

1) JVM options for heap dump on OOME

Add JVM flags (use whichever env var your image honors—common ones are JAVA_TOOL_OPTIONS, JAVA_OPTS, or app-specific):

-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/heapdumps
-XX:+ExitOnOutOfMemoryError
# Container-aware sizing (recommended):
-XX:MaxRAMPercentage=75
-XX:InitialRAMPercentage=50


Notes:

HeapDumpOnOutOfMemoryError dumps only when the JVM throws java.lang.OutOfMemoryError. If the pod is OOM-Killed by the kernel because the container hits the memory limit, you won’t get a dump. See checklist in §6.

Set HeapDumpPath to a mounted, persistent path (we’ll mount /heapdumps).

Keep headroom: set pod memory limit comfortably above Xmx (or use MaxRAMPercentage as above).

2) Check what persistent storage you have
# StorageClasses available (what you can dynamically provision)
oc get storageclass

# Any PVCs already in your namespace?
oc get pvc

# Existing PVs (usually managed by the cluster; fyi)
oc get pv

# Describe a specific StorageClass (to see reclaim policy, volumeBindingMode, etc.)
oc describe storageclass <name>


Pick a StorageClass that supports ReadWriteOnce (RWO) for one pod writer (most block volumes) or ReadWriteMany (RWX) if you want multiple pods to read/write (e.g., NFS/cephfs).

3) Create a PersistentVolumeClaim (PVC)

Example PVC (replace STANDARD-SC with your StorageClass, size as needed):

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: heapdump-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: STANDARD-SC
  resources:
    requests:
      storage: 20Gi


Apply:

oc apply -f heapdump-pvc.yaml
oc get pvc heapdump-pvc

4) Mount PVC and set JVM options in your Deployment

Minimal patch-style Deployment snippet:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-java-app
spec:
  replicas: 1
  template:
    spec:
      # OpenShift often runs containers with a random UID; fsGroup lets the pod write to the volume.
      securityContext:
        fsGroup: 2000
      containers:
        - name: app
          image: your/registry:tag
          env:
            # Change to JAVA_OPTS or app-specific if needed
            - name: JAVA_TOOL_OPTIONS
              value: >
                -XX:+HeapDumpOnOutOfMemoryError
                -XX:HeapDumpPath=/heapdumps
                -XX:+ExitOnOutOfMemoryError
                -XX:MaxRAMPercentage=75
                -XX:InitialRAMPercentage=50
          volumeMounts:
            - name: heapdumps
              mountPath: /heapdumps
          # Example resources with headroom (tune for your app)
          resources:
            requests:
              memory: "1Gi"
              cpu: "250m"
            limits:
              memory: "2Gi"
              cpu: "1"
      volumes:
        - name: heapdumps
          persistentVolumeClaim:
            claimName: heapdump-pvc


Apply (replace with your file):

oc apply -f deployment.yaml

5) Verify at runtime
oc get pods
oc describe pod <pod-name> | sed -n '/Events/,$p'
# Inside the pod, ensure the mount exists:
oc exec -it <pod-name> -- sh -lc "ls -l /heapdumps && id"

6) Make sure you actually get a dump (OOMKill vs OOME)

There are two failure modes:

A) Java OOME (good for heap dump):
The app logs show java.lang.OutOfMemoryError: Java heap space (or metaspace). The JVM writes the dump to /heapdumps.

B) Container OOMKill (no Java dump):
oc describe pod shows Last State: Terminated with Reason: OOMKilled. The kernel kills the process before Java can dump. To avoid this:

Ensure Xmx (or MaxRAMPercentage) + native/misc overhead < pod memory limit.

Increase the pod memory limit or lower JVM heap.

Add monitoring for RSS vs limit.
You can also proactively trigger a dump on demand before the pod gets killed:

# Trigger a live dump manually when memory climbs:
oc exec <pod> -- jcmd 1 GC.heap_dump /heapdumps/manual-$(date +%s).hprof


(Adjust PID; many images have jcmd; if not, include a JDK or a sidecar with JDK tools that can jattach.)

7) Retrieve the heap dump
# From pod to your machine:
oc cp <pod-name>:/heapdumps ./heapdumps
# Or rsync (nice for big files):
oc rsync <pod-name>:/heapdumps ./heapdumps-local

8) Extras & good practices

File size: Heap dumps can be large (often ≈ Xmx). Size your PVC accordingly (and set a Resources limit on the volume if your storage supports quotas).

File naming: JVM will typically name dumps like java_pid<pid>.hprof. You can point HeapDumpPath to a directory (as above) or to a file (e.g., /heapdumps/dump.hprof). A directory is safer for multiple dumps.

Security context: If you see permission issues on the mounted path, keep fsGroup (as above). Avoid setting fixed runAsUser in OpenShift unless you know your SCCs; the random UID model is safer, and fsGroup grants write access.

CrashLoop people: If you’re already crash-looping, you can temporarily remove/raise memory limits or add command: ["sh","-c","sleep 3600 && exec <your-cmd>"] to get a shell window to inspect and test jcmd.

Java metaspace OOME: The same flag dumps heaps on metaspace OOME too, but you may also monitor metaspace growth (-XX:MaxMetaspaceSize if needed).

If you paste your current Deployment (redacted), I can tailor the exact patch (env var names, resources, and storage class) to your setup.

## CSV to JSON

import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.dataformat.csv.*;

import java.io.File;
import java.util.*;

public class Csv2Json {
  public static void main(String[] args) throws Exception {
    File in = new File("data.csv");
    File out = new File("data.json");

    CsvMapper csv = new CsvMapper();
    CsvSchema schema = CsvSchema.emptySchema().withHeader(); // use first row as header

    // rows become Map<header, value>
    MappingIterator<Map<String, String>> it =
        csv.readerFor(Map.class).with(schema).readValues(in);

    List<Map<String, String>> rows = it.readAll();

    ObjectMapper json = new ObjectMapper();
    json.writerWithDefaultPrettyPrinter().writeValue(out, rows);
  }
}

<dependency>
  <groupId>com.fasterxml.jackson.core</groupId>
  <artifactId>jackson-databind</artifactId>
  <version>2.18.1</version>
</dependency>
<dependency>
  <groupId>com.fasterxml.jackson.dataformat</groupId>
  <artifactId>jackson-dataformat-csv</artifactId>
  <version>2.18.1</version>
</dependency>

## SCRIPT RUNNER

# Regex for refs (what Bitbucket actually sees: refs/heads/...):

^refs/heads/((
  (feature|bugfix|hotfix|chore)/
    [A-Z][A-Z0-9]+-[0-9]+        # JIRA key
    (-[a-z0-9-]+)*               # optional extra dash-separated words
)|
  (experiment/[a-z0-9._-]+/[a-z0-9._-]+)
)$

# Regex groovy

def allowed = ~/^refs\/heads\/(((feature|bugfix|hotfix|chore)\/[A-Z][A-Z0-9]+-[0-9]+(-[a-z0-9-]+)*)|(experiment\/[a-z0-9._-]+\/[a-z0-9._-]+))$/

# pre-receive

import com.atlassian.bitbucket.hook.repository.*
import com.atlassian.bitbucket.repository.RefChangeType

// Allowed branch name patterns
def allowedPattern = ~/^refs\/heads\/(((feature|bugfix|hotfix|chore)\/[A-Z][A-Z0-9]+-[0-9]+(-[a-z0-9-]+)*)|(experiment\/[a-z0-9._-]+\/[a-z0-9._-]+))$/

// refChanges is injected by ScriptRunner
for (def refChange : refChanges) {
    def refId = refChange.ref.id as String

    // Only validate branches
    if (!refId.startsWith("refs/heads/")) {
        continue
    }

    // Validate against our naming convention
    if (!(refId ==~ allowedPattern)) {
        def summary = "Branch naming convention violated"
        def details = """\
Branch '${refId}' does not follow the required naming convention.

Allowed patterns:
  - feature|bugfix|hotfix|chore/JIRA-123[-description]
  - experiment/<user>/<description>

Examples:
  - refs/heads/feature/ABC-123-add-new-risk-limit
  - refs/heads/bugfix/RISK-456-null-pointer
  - refs/heads/experiment/lukasz/new-risk-model
"""
        return RepositoryHookResult.rejected(summary, details)
    }
}

return RepositoryHookResult.accepted()

# merge check

import com.atlassian.bitbucket.pull.PullRequest

// pullRequest is provided by ScriptRunner
String sourceBranch = pullRequest.fromRef.displayId  // e.g. 'experiment/lukasz/new-risk-model'
String targetBranch = pullRequest.toRef.displayId    // e.g. 'main' or 'develop'

// Branches we treat as protected "real" branches
def protectedTargets = ['main', 'master', 'develop']

boolean isExperiment = sourceBranch.startsWith("experiment/")
boolean isProtectedTarget = protectedTargets.contains(targetBranch)

if (isExperiment && isProtectedTarget) {
    mergeRequest.reject(
        "Experimental branches cannot be merged into ${targetBranch}",
        """\
The branch '${sourceBranch}' is marked as experimental.

Experimental branches are not intended for direct merge into '${targetBranch}'.
If you want to bring this work into the main codebase:
  1. Create a proper feature/BUG branch with a JIRA key, and
  2. Cherry-pick or re-implement relevant commits there.
"""
    )
    return false
}

// otherwise allow
return true

# local hook

#!/usr/bin/env bash

BRANCH_NAME=$(git rev-parse --abbrev-ref HEAD)

# Same convention, but without refs/heads/ prefix
BRANCH_REGEX='^((feature|bugfix|hotfix|chore)/[A-Z][A-Z0-9]+-[0-9]+(-[a-z0-9-]+)*|experiment/[a-z0-9._-]+/[a-z0-9._-]+)$'

if [[ ! "$BRANCH_NAME" =~ $BRANCH_REGEX ]]; then
  echo "ERROR: Branch name '$BRANCH_NAME' does not follow naming convention."
  echo "Allowed:"
  echo "  - feature|bugfix|hotfix|chore/JIRA-123[-description]"
  echo "  - experiment/<user>/<description>"
  exit 1
fi

## commit message check

import com.atlassian.bitbucket.hook.repository.CommitAddedDetails
import com.atlassian.bitbucket.hook.repository.PreRepositoryHookCommitCallback
import com.atlassian.bitbucket.hook.repository.RepositoryHookCommitFilter
import com.atlassian.bitbucket.hook.repository.RepositoryHookResult

import javax.annotation.Nonnull

// --------------- CONFIGURATION SECTION ------------------

// Branch patterns where commit message check is NOT enforced
// Use full ref IDs (refs/heads/...), matched with regex.
def excludedRefPatterns = [
    ~/^refs\/heads\/experiment\/.*/,    // skip experiment/* branches
    // ~/^refs\/heads\/sandbox\/.*/,    // example: skip sandbox/*
]

// JIRA key at beginning of message, e.g. "RISK-123 ..." or "APP2-9: ..."
// - optional leading whitespace
// - key = uppercase letters + digits, dash, number
// Adjust if you need different JIRA key style.
def jiraPrefixPattern = ~/^\s*[A-Z][A-Z0-9]+-[0-9]+\b.*/

// --------------- IMPLEMENTATION -------------------------

commitCallback =
    new PreRepositoryHookCommitCallback() {

        @Override
        boolean onCommitAdded(@Nonnull CommitAddedDetails commitDetails) {
            def commit = commitDetails.commit
            def message = commit.message ?: ""
            def refId   = commitDetails.ref.id as String   // e.g. refs/heads/feature/RISK-123-something

            // 1) Skip check on excluded branches
            boolean isExcludedRef = excludedRefPatterns.any { pattern ->
                refId ==~ pattern
            }
            if (isExcludedRef) {
                return true    // do not validate this commit, continue with next
            }

            // 2) Enforce commit prefix: JIRA key at start of message
            def matchesPrefix = (message =~ jiraPrefixPattern).find()

            if (!matchesPrefix) {
                // Block push and show clear error
                resultBuilder.veto(
                    "Invalid commit message",
                    """Commit ${commit.displayId} on branch '${refId}' has an invalid message:

'${message}'

Expected format:
  - Message must start with a JIRA issue key, e.g.
    RISK-123 Implement new exposure limit
    APP2-9: Fix timeout in risk engine

If this is an experimental/POC branch, use a branch name that matches one of the excluded patterns
(e.g. refs/heads/experiment/...)."""
                )
                return false   // stop processing further commits, veto push
            }

            // OK → continue with next commit
            return true
        }

        @Override
        RepositoryHookResult getResult() {
            // Build final result (accept or reject) based on vetos above
            resultBuilder.build()
        }
    }

// Check commit messages for all commits added to any ref
commitFilters << RepositoryHookCommitFilter.ADDED_TO_ANY_REF

// The actual decision (accept/veto) is made in commitCallback
return RepositoryHookResult.accepted()

## SC condition excluding branches from commit mesage check

// Patterns for branches to exclude from Jira commit validation
def excludedPatterns = [
    ~/^refs\/heads\/adhoc\/.*/, 
    ~/^refs\/heads\/poc\/.*/
]

// Check all refs being pushed
boolean anyExcluded = refChanges.any { refChange ->
    def fullRef = refChange.ref.id  // e.g. "refs/heads/feature/ABC-1"
    excludedPatterns.any { pattern -> fullRef ==~ pattern }
}

// If ANY of the pushed refs matches an excluded pattern -> skip hook
return !anyExcluded


